Metadata-Version: 2.1
Name: mice
Version: 0.1.30
Summary: Multi-iteration Stochastic Estimator
Home-page: https://bitbucket.org/agcarlon/mice
Author: Andre Gustavo Carlon
Author-email: agcarlon@gmail.com
License: GPLv3
Project-URL: Documentation, https://mice.readthedocs.io/
Project-URL: Source, https://bitbucket.org/agcarlon/mice
Project-URL: Manuscript, https://arxiv.org/abs/2011.01718
Keywords: stochastic optimization,hierarchical methods,Monte Carlo
Platform: UNKNOWN
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Information Technology
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Education
Classifier: Topic :: Utilities
Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3 :: Only
Description-Content-Type: text/x-rst
License-File: LICENSE
Requires-Dist: numpy (>=1.19)
Requires-Dist: scipy (>=1.8.1)
Requires-Dist: pandas (>=1.1)
Requires-Dist: matplotlib (>=3.2.0)

Multi-iteration Stochastic Estimator
------------------------------------

The `Multi-Iteration stochastiC Estimator`_ (MICE) is an estimator of gradients to be used in stochastic optimization. It uses control variates to build a hierarchy of iterations, adaptively sampling to keep the statistical variance below tolerance in an optimal fashion, cost-wise. The tolerance on the statistical error decreases proportionally to the square of the gradient norm, thus, SGD-MICE converges linearly in strongly convex L-smooth functions.

.. _Multi-Iteration stochastiC Estimator: https://arxiv.org/abs/2011.01718

This python implementation of MICE is able to

* estimate expectations or finite sums of gradients of functions;

* choose the optimal sample sizes in order to minimize the sampling cost;

* build a hierarchy of iterations that minimizes the total work;

* use a resampling technique to compute the gradient norm, thus enforcing stability;

* define a tolerance on the norm of the gradient estimate or a maximum number of evaluations as a stopping criterion.

Using MICE
----------

Using MICE is as simple as

    >>> import numpy as np
    >>> from mice import MICE
    >>>
    >>>
    >>> def gradient(x, thts):
    >>>     return x - thts
    >>>
    >>>
    >>> def sampler(n):
    >>>     return np.random.random((n, 1))
    >>>
    >>>
    >>> df = MICE(gradient , sampler=sampler)
    >>> x = 10
    >>> for i in range(10):
    ...    grad = df(x)
    ...    x = x - grad


However, it is flexible enough to tackle more complex problems.
For more information on how to use MICE and examples, check the `documentation`_.

.. _documentation: https://mice.readthedocs.io

