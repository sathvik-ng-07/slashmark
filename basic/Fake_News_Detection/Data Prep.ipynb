{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bd3e812-d0a9-4065-ad9b-e871ee2c3d13",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SnowballStemmer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3cd6c6-34b5-42e1-abaf-42e90b9dc60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = 'test.csv'\n",
    "train_filename = 'train.csv'\n",
    "valid_filename = 'valid.csv'\n",
    "\n",
    "train_news = pd.read_csv(train_filename)\n",
    "test_news = pd.read_csv(test_filename)\n",
    "valid_news = pd.read_csv(valid_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bca841-8dc8-4684-b56a-39d3ce2b76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7cb796-f190-4c81-8ef0-ce3b9a908686",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da1d43-9b1b-4d9a-97f6-b22e14b6bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eac180-1389-4538-a0dd-55edda1fc41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87388c-0f2b-4c34-8c32-cd2f3ee48d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad880d7-78ae-401c-9c86-a77455d80abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfb8c9-c2d5-4319-8c2b-b7221285aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure and axes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Assuming train_news, test_news, and valid_news are your datasets\n",
    "sns.countplot(x='Label', data=train_news, hue='Label', palette='hls', ax=axes[0], legend=False)\n",
    "axes[0].set_title('Train Data Distribution')\n",
    "\n",
    "sns.countplot(x='Label', data=test_news, hue='Label', palette='hls', ax=axes[1], legend=False)\n",
    "axes[1].set_title('Test Data Distribution')\n",
    "\n",
    "sns.countplot(x='Label', data=valid_news, hue='Label', palette='hls', ax=axes[2], legend=False)\n",
    "axes[2].set_title('Validation Data Distribution')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a72af-d562-4705-96bc-e7ef67d02b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80e229-3585-42f7-bb5b-97597e731116",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7174a-cd2f-40f0-9ec8-ddd1514294c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_news.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2215fad-7836-4b88-9d35-9a2672919422",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f79ac5-8307-4057-a761-94d91142e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17608c04-1c9e-453e-9caa-3175e649bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62680e85-7dbc-4577-9747-b161c0c21847",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stemmer = SnowballStemmer('english')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "list(stopwords)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba6dc2-4276-4cfc-b8af-453c54d3e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for token in tokens:\n",
    "        stemmed.append(stemmer.stem(token))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af68dad-2994-44ab-9042-82171496dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the data\n",
    "def process_data(data,exclude_stopword=True,stem=True):\n",
    "    tokens = [w.lower() for w in data]\n",
    "    tokens_stemmed = tokens\n",
    "    tokens_stemmed = stem_tokens(tokens, eng_stemmer)\n",
    "    tokens_stemmed = [w for w in tokens_stemmed if w not in stopwords ]\n",
    "    return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50050be0-e374-40b0-86ac-e779e9ce04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating ngrams\n",
    "#unigram \n",
    "def create_unigram(words):\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "\n",
    "#bigram\n",
    "def create_bigrams(words):\n",
    "    assert type(words) == list\n",
    "    skip = 0\n",
    "    join_str = \" \"\n",
    "    Len = len(words)\n",
    "    if Len > 1:\n",
    "        lst = []\n",
    "        for i in range(Len-1):\n",
    "            for k in range(1,skip+2):\n",
    "                if i+k < Len:\n",
    "                    lst.append(join_str.join([words[i],words[i+k]]))\n",
    "    else:\n",
    "        #set it as unigram\n",
    "        lst = create_unigram(words)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1577c8-54b2-4241-8433-6d3c04ad95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334227e7-a2ad-4070-9ceb-a11a359fe1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
